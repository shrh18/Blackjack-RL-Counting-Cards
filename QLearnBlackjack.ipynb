{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import gymnasium as gym\n",
    "\n",
    "class BlackjackQLearningAgent:\n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.95, epsilon=1.0, epsilon_decay=0.99999, min_epsilon=0.01):\n",
    "        self.q_values = defaultdict(lambda: np.zeros(2))\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice([0, 1])\n",
    "        else:\n",
    "            action = np.argmax(self.q_values[state])\n",
    "        \n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "        return action\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        current_q = self.q_values[state][action]\n",
    "        if done:\n",
    "            target_q = reward\n",
    "        else:\n",
    "            target_q = reward + self.gamma * np.max(self.q_values[next_state])\n",
    "        self.q_values[state][action] += self.lr * (target_q - current_q)\n",
    "\n",
    "class BlackjackEnv(gym.Env):\n",
    "    def __init__(self, render_mode=None, natural=False, sab=False, total_decks=5):\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Tuple(\n",
    "            (gym.spaces.Discrete(32), gym.spaces.Discrete(11), gym.spaces.Discrete(2))\n",
    "        )\n",
    "        self.natural = natural\n",
    "        self.sab = sab\n",
    "        self.render_mode = render_mode\n",
    "        self.running_count = 0\n",
    "        self.betting_unit = 1\n",
    "        self.money = 50\n",
    "        self.current_bet = 1\n",
    "        one_suite = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10]\n",
    "        self.original_deck = one_suite * 4 * total_decks\n",
    "        self.deck = self.original_deck.copy()\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        if action:\n",
    "            self.player.append(self.draw_card(self.np_random))\n",
    "            if self.is_bust(self.player):\n",
    "                terminated = True\n",
    "                reward = -1.0\n",
    "            else:\n",
    "                terminated = False\n",
    "                reward = 0.0\n",
    "        else:\n",
    "            terminated = True\n",
    "            while self.sum_hand(self.dealer) < 17:\n",
    "                self.dealer.append(self.draw_card(self.np_random))\n",
    "            reward = self.cmp(self.score(self.player), self.score(self.dealer))\n",
    "            if self.sab and self.is_natural(self.player) and not self.is_natural(self.dealer):\n",
    "                reward = 1.0\n",
    "            elif not self.sab and self.natural and self.is_natural(self.player) and reward == 1.0:\n",
    "                reward = 1.5\n",
    "\n",
    "        self.money += reward * self.current_bet\n",
    "        return self._get_obs(), reward, terminated, self.money\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return (self.sum_hand(self.player), self.dealer[0], self.usable_ace(self.player), self.running_count, self.getRemainingDecks())\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.money = 50\n",
    "        self.deck = self.original_deck.copy()\n",
    "        self.running_count = 0\n",
    "        return self.new_round()\n",
    "\n",
    "    def new_round(self):\n",
    "        self.dealer = self.draw_hand(self.np_random)\n",
    "        self.player = self.draw_hand(self.np_random)\n",
    "        return self._get_obs()\n",
    "\n",
    "    def draw_card(self, np_random):\n",
    "        card_index = np_random.choice(len(self.deck))\n",
    "        card = self.deck[card_index]\n",
    "        if card in [1, 10]:\n",
    "            self.running_count -= 1\n",
    "        elif 2 <= card <= 6:\n",
    "            self.running_count += 1\n",
    "        self.deck.pop(card_index)\n",
    "        return card\n",
    "\n",
    "    def draw_hand(self, np_random):\n",
    "        return [self.draw_card(np_random), self.draw_card(np_random)]\n",
    "\n",
    "    def getRemainingDecks(self):\n",
    "        return round(len(self.deck) / 52 * 2) / 2\n",
    "\n",
    "    @staticmethod\n",
    "    def cmp(a, b):\n",
    "        return float(a > b) - float(a < b)\n",
    "\n",
    "    @staticmethod\n",
    "    def usable_ace(hand):\n",
    "        return 1 in hand and sum(hand) + 10 <= 21\n",
    "\n",
    "    @staticmethod\n",
    "    def sum_hand(hand):\n",
    "        if BlackjackEnv.usable_ace(hand):\n",
    "            return sum(hand) + 10\n",
    "        return sum(hand)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_bust(hand):\n",
    "        return BlackjackEnv.sum_hand(hand) > 21\n",
    "\n",
    "    @staticmethod\n",
    "    def score(hand):\n",
    "        return 0 if BlackjackEnv.is_bust(hand) else BlackjackEnv.sum_hand(hand)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_natural(hand):\n",
    "        return sorted(hand) == [1, 10]\n",
    "\n",
    "# def train_agent(env, agent, num_episodes):\n",
    "#     for episode in range(num_episodes):\n",
    "#         if env.getRemainingDecks() < 1 or env.money < 1:\n",
    "#             state = env.reset()\n",
    "#         else:\n",
    "#             state = env.new_round()\n",
    "#         done = False\n",
    "#         while not done:\n",
    "#             action = agent.get_action(state)\n",
    "#             next_state, reward, done, _ = env.step(action)\n",
    "#             agent.update(state, action, reward, next_state, done)\n",
    "#             state = next_state\n",
    "#         if episode % 10000 == 0:\n",
    "#             print(f\"Episode {episode} completed\")\n",
    "\n",
    "def train_agent(env, agent, num_episodes):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        while env.getRemainingDecks() >= 1 and env.money >= 1:\n",
    "            state = env.new_round()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = agent.get_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                agent.update(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "        if episode % 10000 == 0:\n",
    "            print(f\"Episode {episode}Â completed\")\n",
    "\n",
    "def evaluate_agent(env, agent, num_episodes=10000):\n",
    "    total_reward = 0\n",
    "    env.reset()\n",
    "    for _ in range(num_episodes):\n",
    "        if env.getRemainingDecks() < 1 or env.money < 1:\n",
    "            state = env.reset()\n",
    "        else:\n",
    "            state = env.new_round()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.argmax(agent.q_values[state])\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "    return total_reward / num_episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 completed\n",
      "Episode 10000 completed\n",
      "Episode 20000 completed\n",
      "Episode 30000 completed\n",
      "Episode 40000 completed\n",
      "Episode 50000 completed\n",
      "Episode 60000 completed\n",
      "Episode 70000 completed\n",
      "Episode 80000 completed\n",
      "Episode 90000 completed\n",
      "Average reward over 10,000 episodes: -0.1337\n",
      "intital money =  47.0\n",
      "Iteration- 0 : (18, 8, False, -10, 3.0) 0.0 False 47.0 action taken =  1\n",
      "Iteration- 0 : (18, 8, False, -10, 3.0) -1.0 True 46.0 action taken =  0\n",
      "Iteration- 1 : (25, 6, False, -9, 3.0) -1.0 True 45.0 action taken =  1\n",
      "Iteration- 2 : (17, 5, False, -5, 2.5) 0.0 False 45.0 action taken =  1\n",
      "Iteration- 2 : (20, 5, False, -4, 2.5) 0.0 False 45.0 action taken =  1\n",
      "Iteration- 2 : (20, 5, False, -1, 2.5) 1.0 True 46.0 action taken =  0\n",
      "Iteration- 3 : (21, 2, True, 1, 2.5) 0.0 True 46.0 action taken =  0\n",
      "Iteration- 4 : (22, 9, False, 1, 2.5) -1.0 True 45.0 action taken =  1\n",
      "Iteration- 5 : (20, 8, False, 3, 2.5) 0.0 False 45.0 action taken =  1\n",
      "Iteration- 5 : (20, 8, False, 2, 2.5) 0.0 True 45.0 action taken =  0\n",
      "Iteration- 6 : (13, 10, False, 0, 2.0) 0.0 False 45.0 action taken =  1\n",
      "Iteration- 6 : (18, 10, False, 1, 2.0) 0.0 False 45.0 action taken =  1\n",
      "Iteration- 6 : (28, 10, False, 0, 2.0) -1.0 True 44.0 action taken =  1\n",
      "Iteration- 7 : (14, 5, False, 1, 2.0) -1.0 True 43.0 action taken =  0\n",
      "Iteration- 8 : (26, 10, False, -1, 2.0) -1.0 True 42.0 action taken =  1\n",
      "Iteration- 9 : (13, 9, False, -2, 2.0) 1.0 True 43.0 action taken =  0\n"
     ]
    }
   ],
   "source": [
    "# Create environment and agent\n",
    "env = BlackjackEnv()\n",
    "agent = BlackjackQLearningAgent()\n",
    "\n",
    "# Train the agent\n",
    "train_agent(env, agent, num_episodes=100000)\n",
    "\n",
    "# Evaluate the agent\n",
    "env.reset()\n",
    "average_reward = evaluate_agent(env, agent)\n",
    "print(f\"Average reward over 10,000 episodes: {average_reward}\")\n",
    "\n",
    "# money = env.money\n",
    "# print(\"intital money = \", money)\n",
    "# for i in range(10):\n",
    "#     done = False\n",
    "#     observation = env.new_round()  # starts a new round\n",
    "#     while not done:\n",
    "#         action = env.action_space.sample()\n",
    "#         observation, reward, done, money = env.step(action)\n",
    "#         print(\"Iteration-\",i, \":\",observation, reward, done, money, \"action taken = \", action)\n",
    "\n",
    "money = env.money\n",
    "print(\"intital money = \", money)\n",
    "for i in range(10):\n",
    "    done = False\n",
    "    observation = env.new_round()\n",
    "    # observation = env.reset()\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, money = env.step(action)\n",
    "        print(\"Iteration-\", i, \":\",observation, reward, done, money, \"action taken = \", action)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
