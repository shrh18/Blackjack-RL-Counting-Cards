{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fecbe5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent...\n",
      "Episode 0 completed\n",
      "Episode 10000 completed\n",
      "Episode 20000 completed\n",
      "Episode 30000 completed\n",
      "Episode 40000 completed\n",
      "Episode 50000 completed\n",
      "Episode 60000 completed\n",
      "Episode 70000 completed\n",
      "Episode 80000 completed\n",
      "Episode 90000 completed\n",
      "Evaluating agent...\n",
      "Average reward over 10,000 episodes: -0.0403\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import gymnasium as gym\n",
    "\n",
    "class BlackjackQLearningAgent:\n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.95, epsilon=1.0, epsilon_decay=0.99999, min_epsilon=0.01):\n",
    "        self.q_values = defaultdict(lambda: np.zeros(2))  # Initialize Q-values for the actions (hit, stick)\n",
    "        self.lr = learning_rate  # Learning rate\n",
    "        self.gamma = discount_factor  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration probability\n",
    "        self.epsilon_decay = epsilon_decay  # Decay factor for epsilon\n",
    "        self.min_epsilon = min_epsilon  # Minimum epsilon value to avoid zero exploration\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice([0, 1])  # 0 = stick, 1 = hit (action space of Blackjack)\n",
    "        else:\n",
    "            action = np.argmax(self.q_values[state])  # Choose action with highest Q-value\n",
    "        \n",
    "        # Decay epsilon for exploration-exploitation trade-off\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "        return action\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Update the Q-table using the Q-learning update rule.\"\"\"\n",
    "        current_q = self.q_values[state][action]\n",
    "        if done:\n",
    "            target_q = reward\n",
    "        else:\n",
    "            target_q = reward + self.gamma * np.max(self.q_values[next_state])  # Bellman equation\n",
    "        # Update the Q-value for the current state-action pair\n",
    "        self.q_values[state][action] += self.lr * (target_q - current_q)\n",
    "\n",
    "\n",
    "class BlackjackEnv(gym.Env):\n",
    "    def __init__(self, render_mode=None, natural=False, sab=False, total_decks=5):\n",
    "        # Action space (0 = stick, 1 = hit)\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        # Observation space: (player sum, dealer showing card, usable ace, running count, remaining decks)\n",
    "        self.observation_space = gym.spaces.Tuple(\n",
    "            (gym.spaces.Discrete(32), gym.spaces.Discrete(11), gym.spaces.Discrete(2))\n",
    "        )\n",
    "        self.natural = natural\n",
    "        self.sab = sab\n",
    "        self.render_mode = render_mode\n",
    "        self.running_count = 0\n",
    "        self.betting_unit = 1\n",
    "        self.money = 50\n",
    "        self.current_bet = 1\n",
    "        one_suite = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10]\n",
    "        self.original_deck = one_suite * 4 * total_decks\n",
    "        self.deck = self.original_deck.copy()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and return the next state, reward, done, and additional info.\"\"\"\n",
    "        assert self.action_space.contains(action)\n",
    "        if action:  # If action is hit (1)\n",
    "            self.player.append(self.draw_card(self.np_random))\n",
    "            if self.is_bust(self.player):\n",
    "                terminated = True\n",
    "                reward = -1.0\n",
    "            else:\n",
    "                terminated = False\n",
    "                reward = 0.0\n",
    "        else:  # If action is stick (0)\n",
    "            terminated = True\n",
    "            while self.sum_hand(self.dealer) < 17:  # Dealer must hit until their sum is at least 17\n",
    "                self.dealer.append(self.draw_card(self.np_random))\n",
    "            reward = self.cmp(self.score(self.player), self.score(self.dealer))\n",
    "            if self.sab and self.is_natural(self.player) and not self.is_natural(self.dealer):\n",
    "                reward = 1.0\n",
    "            elif not self.sab and self.natural and self.is_natural(self.player) and reward == 1.0:\n",
    "                reward = 1.5\n",
    "\n",
    "        # Update money based on game outcome\n",
    "        self.money += reward * self.current_bet\n",
    "        return self._get_obs(), reward, terminated, self.money\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"Return the current observation.\"\"\"\n",
    "        return (self.sum_hand(self.player), self.dealer[0], self.usable_ace(self.player), self.running_count, self.getRemainingDecks())\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"Reset the environment to start a new round.\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        self.money = 50\n",
    "        self.deck = self.original_deck.copy()\n",
    "        self.running_count = 0\n",
    "        return self.new_round()\n",
    "\n",
    "    def new_round(self):\n",
    "        \"\"\"Start a new round of blackjack.\"\"\"\n",
    "        self.dealer = self.draw_hand(self.np_random)\n",
    "        self.player = self.draw_hand(self.np_random)\n",
    "        return self._get_obs()\n",
    "\n",
    "    def draw_card(self, np_random):\n",
    "        \"\"\"Draw a card from the deck.\"\"\"\n",
    "        card_index = np_random.choice(len(self.deck))\n",
    "        card = self.deck[card_index]\n",
    "        if card in [1, 10]:\n",
    "            self.running_count -= 1\n",
    "        elif 2 <= card <= 6:\n",
    "            self.running_count += 1\n",
    "        self.deck.pop(card_index)\n",
    "        return card\n",
    "\n",
    "    def draw_hand(self, np_random):\n",
    "        \"\"\"Draw two cards for a hand.\"\"\"\n",
    "        return [self.draw_card(np_random), self.draw_card(np_random)]\n",
    "\n",
    "    def getRemainingDecks(self):\n",
    "        \"\"\"Return the number of remaining decks in the shoe.\"\"\"\n",
    "        return round(len(self.deck) / 52 * 2) / 2\n",
    "\n",
    "    @staticmethod\n",
    "    def cmp(a, b):\n",
    "        \"\"\"Compare player and dealer scores.\"\"\"\n",
    "        return float(a > b) - float(a < b)\n",
    "\n",
    "    @staticmethod\n",
    "    def usable_ace(hand):\n",
    "        \"\"\"Check if the hand contains a usable ace.\"\"\"\n",
    "        return 1 in hand and sum(hand) + 10 <= 21\n",
    "\n",
    "    @staticmethod\n",
    "    def sum_hand(hand):\n",
    "        \"\"\"Return the sum of the hand.\"\"\"\n",
    "        if BlackjackEnv.usable_ace(hand):\n",
    "            return sum(hand) + 10\n",
    "        return sum(hand)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_bust(hand):\n",
    "        \"\"\"Check if the hand is a bust (greater than 21).\"\"\"\n",
    "        return BlackjackEnv.sum_hand(hand) > 21\n",
    "\n",
    "    @staticmethod\n",
    "    def score(hand):\n",
    "        \"\"\"Return the score of the hand.\"\"\"\n",
    "        return 0 if BlackjackEnv.is_bust(hand) else BlackjackEnv.sum_hand(hand)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_natural(hand):\n",
    "        \"\"\"Check if the hand is a natural blackjack (Ace + 10-value card).\"\"\"\n",
    "        return sorted(hand) == [1, 10]\n",
    "\n",
    "\n",
    "def train_agent(env, agent, num_episodes):\n",
    "    \"\"\"Train the Q-learning agent.\"\"\"\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            # Check if money is 0 or remaining decks are less than 1\n",
    "            if env.money <= 0 or env.getRemainingDecks() < 1:\n",
    "                env.reset()  # Reset environment if money is 0 or decks are less than 1\n",
    "                break\n",
    "        \n",
    "        if episode % 10000 == 0:\n",
    "            print(f\"Episode {episode} completed\")\n",
    "\n",
    "\n",
    "def evaluate_agent(env, agent, num_episodes=10000):\n",
    "    \"\"\"Evaluate the trained agent.\"\"\"\n",
    "    total_reward = 0\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.argmax(agent.q_values[state])\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "    return total_reward / num_episodes\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = BlackjackEnv()\n",
    "    agent = BlackjackQLearningAgent()\n",
    "    \n",
    "    # Train the agent\n",
    "    num_episodes = 100000\n",
    "    print(\"Training agent...\")\n",
    "    train_agent(env, agent, num_episodes)\n",
    "    \n",
    "    # Test the agent after training\n",
    "    print(\"Evaluating agent...\")\n",
    "    avg_reward = evaluate_agent(env, agent)\n",
    "    print(f\"Average reward over 10,000 episodes: {avg_reward:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1218905e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
